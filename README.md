# Twitter_Analysis
The below task is one of the Neural Network and Deep learning class task and other details are mentioned below:

Aim: 
To learn Web scraping, Data pre-processing & cleaning, PCA Analysis and successfully running a model on the scaled data. :hugs:

Description: The Indetail process I have followed in this task is described below with the terminology description 	:face_in_clouds:

i. Web Scraping: Web scraping refers to the extraction of data from a website. This information is collected and then exported into a format that is more useful for the user. Be it a spreadsheet or an API. This is step 1 of the task where we got the data from web and turned to a dataframe

In Step 2 we cleaned the text which is considered as data pre-processing

ii. PCA: Principal Component Analysis, or PCA, is a dimensionality-reduction approach for reducing the dimensionality of large data sets by transforming a large collection of variables into a smaller one that retains the majority of the information in the large set. This is step 3 and we used this to scale the data.

iii. Scaled Data: This means that we're transforming out data so that it fits within a specific scale, like 0-100 or 0-1. we want to scale data when we're using methods based on measures of how far apart data points, like support vector machines, or SVM or k-nearest neighbors, or KNN. This is step 4 and the scaled data is ready for modeling.

iv: K-means Clustering: k-means Clustering is a vector quantization method that seeks to split n observations into k clusters, with each observation belonging to the cluster with the closest mean (cluster centres or cluster centroid), which serves as the cluster's prototype. This is the final step and the clusters are defined and the observations can be made.

Thanks for scrolling! Happy Coding :) Reach me at linkedIn @Harjot Parhar www.linkedin.com/in/harjot-parhar  	:ghost:
